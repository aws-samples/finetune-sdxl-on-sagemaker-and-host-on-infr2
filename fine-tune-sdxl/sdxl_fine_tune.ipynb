{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8401dc3-5c93-4302-af92-67a7acc904c3",
   "metadata": {},
   "source": [
    "## SDXL Fine Tuning\n",
    "\n",
    "This is tested on SageMaker notebook instance using `conda_pytorch_p310` kernel\n",
    "### Setup\n",
    "\n",
    "Install the required libraries   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b78e-334d-43b2-b7ad-5dc6a48376d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export PIP_ROOT_USER_ACTION=ignore\n",
    "\n",
    "pip install -Uq pip\n",
    "pip install autotrain-advanced==0.6.58\n",
    "pip install diffusers==0.21.4\n",
    "pip install autocrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5dc0ec-173b-444e-8212-18b38a85794e",
   "metadata": {},
   "source": [
    "### > Check version\n",
    "\n",
    "Ensure that GPU devices are available on the system and retrieve information about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3037a11f-37a3-4805-be07-307d7055e113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "NVIDIA A10G\n",
      "Number of available GPU devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__) # e.g., 2.0.0 at time of post\n",
    "\n",
    "print(torch.cuda.get_device_name(0)) # e.g., NVIDIA A10G\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "assert device_count > 0, \"No GPU devices detected.\"\n",
    "\n",
    "print(\"Number of available GPU devices:\", device_count)\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec88ef2-34ea-4249-b1a1-6121224b7e5d",
   "metadata": {},
   "source": [
    "### > Prepare the images. The picture needs to be 1024 x 1024\n",
    "\n",
    "Resize and center-crop each image to a square size of 1024 pixels, and save the processed images in the \"cropped\" directory. \n",
    "If a face cannot be detected in an image, the script skips that image and moves to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10f6f4-9073-4abd-a844-3fbf03bb4de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import utils\n",
    "import shutil\n",
    "\n",
    "imag_dir=Path(\"data\") #source directory to place your image\n",
    "dest_dir = Path(\"cropped\") # destination directory after image processing\n",
    "if dest_dir.exists():\n",
    "    shutil.rmtree(dest_dir)\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for n,img_path in enumerate(chain(imag_dir.glob(\"*.[jJ][pP]*[Gg]\"),imag_dir.glob(\"*.[Pp][Nn][Gg]\"))):\n",
    "    try:\n",
    "        cropped = utils.resize_and_center_crop(img_path.as_posix(), 1024)\n",
    "        cropped.save(dest_dir / f\"image_{n}.png\")\n",
    "    except ValueError:\n",
    "        print(f\"Could not detect face in {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "print(\"Here are the preprocessed images ==========\")\n",
    "[x.as_posix() for x in dest_dir.iterdir() if x.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fd25a-babd-406a-8518-17c38a7b463c",
   "metadata": {},
   "source": [
    "- 8bit adam gobbles the images\n",
    "- prior-preservation exceeds A10G GPU memory\n",
    "- xformers gives package error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d5549-6d7d-48c0-a93f-71585aee80ce",
   "metadata": {},
   "source": [
    "### > Initialize fine tuning parameters\n",
    "\n",
    "Set the base model to Stable Diffusion SDXL and initialize fine tuning hyperparameters\n",
    "instance_prompt is set to <<TOK>> here, but can be set to any unique identifier to personalise the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd2295e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713122b8-e3c7-4ba5-9e50-a59e07deb2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# project configuration\n",
    "project_name = \"finetune_sttirum\"\n",
    "%store project_name\n",
    "\n",
    "model_name_base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "# fine-tuning prompts\n",
    "instance_prompt = \"photo of <<TOK>>\"\n",
    "class_prompt = \"photo of a person\"\n",
    "\n",
    "# fine-tuning hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_steps = 500\n",
    "batch_size = 1\n",
    "gradient_accumulation = 4\n",
    "resolution = 1024\n",
    "num_class_image = 50\n",
    "\n",
    "class_image_path=Path(f\"/tmp/priors\")\n",
    "\n",
    "# environment variables for autotrain command\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name_base\n",
    "os.environ[\"INSTANCE_PROMPT\"] = instance_prompt\n",
    "os.environ[\"CLASS_PROMPT\"] = class_prompt\n",
    "os.environ[\"IMAGE_PATH\"] = dest_dir.as_posix()\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"CLASS_IMAGE_PATH\"] = class_image_path.as_posix()\n",
    "os.environ[\"NUM_CLASS_IMAGE\"] = str(num_class_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb30a1-2c75-47db-96d3-7a229ac3bbde",
   "metadata": {},
   "source": [
    "### > use autotrain to fine tune\n",
    "\n",
    "help command will show all the available parameters\n",
    "\n",
    "```\n",
    "!autotrain dreambooth --help\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af966bb4-0f0b-4245-a789-44cd51b41ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!autotrain dreambooth \\\n",
    "    --model ${MODEL_NAME} \\\n",
    "    --project-name ${PROJECT_NAME} \\\n",
    "    --image-path \"${IMAGE_PATH}\" \\\n",
    "    --prompt \"${INSTANCE_PROMPT}\" \\\n",
    "    --class-prompt \"${CLASS_PROMPT}\" \\\n",
    "    --resolution ${RESOLUTION} \\\n",
    "    --batch-size ${BATCH_SIZE} \\\n",
    "    --num-steps ${NUM_STEPS} \\\n",
    "    --gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "    --lr ${LEARNING_RATE} \\\n",
    "    --fp16 \\\n",
    "    --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baacc39-7596-472a-a5e3-c9b86f245fe2",
   "metadata": {},
   "source": [
    "### > Load the fine tuned model\n",
    "\n",
    "Load the Stable Diffusion XL model with the specified pre-trained weights and fine-tune using the provided LoRA weights. \n",
    "This fine-tuned model can then be used for generating images based on text prompts or other tasks related to stable diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c539cb4e-f37f-4811-b8a0-e2ed6981f605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "project_name = \"finetune_sttirum\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905520e-ba56-45ce-a4e8-eda6a1c4a5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_name_base,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "pipeline.load_lora_weights(\n",
    "    project_name, \n",
    "    weight_name=\"pytorch_lora_weights.safetensors\",\n",
    "    adapter_name=\"sttirum\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdec92",
   "metadata": {},
   "source": [
    "### > Test the fine tuned model\n",
    "\n",
    "Test the fine-tuned model using the unique identifier used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b79947c0-b54d-46ce-b747-c1ffd55f0fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"photo of <<TOK>>, Pixar 3d portrait, ultra detailed, gorgeous, 3d zbrush, trending on dribbble, 8k render\"\"\"\n",
    "negative_prompt = \"\"\"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb454da2-ecaf-44df-b529-eb682a837aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = random.randint(0, 100000)\n",
    "generator = torch.Generator(device).manual_seed(seed)\n",
    "base_image = pipeline(\n",
    "    prompt=prompt, \n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    output_type=\"pil\",\n",
    ").images[0]\n",
    "base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88970a4b-c0eb-442f-9869-bb79ea226cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
